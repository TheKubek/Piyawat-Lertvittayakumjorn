<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Project : Home</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/css/bootstrap.min.css">
    <link rel="stylesheet" href="piyawatStyles.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js"></script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML'
            async></script>
</head>
<body>

<div class="container" id="mainContainer">
    <div class="toppane">
        <div class="pos-f-t" id="littleBar">
            <div class="collapse" id="navbarToggle">
                <div class="navbarContent">
                    <div class="container-fluid">
                        <div class="row">
                            <div class="col-sm-1"></div>
                            <div class="col-sm-2 pageLinks">
                                <a href="index.html">Home</a>
                            </div>
                            <div class="col-sm-2 pageLinks">
                                <a href="nlp.html"><img class="navBarImg" src="Content/Tiles/nlp.png"
                                                        alt="Natural Language Processing"/></a>
                            </div>
                            <div class="col-sm-2 pageLinks">
                                <a href="explainnlp.html"><img class="navBarImg" src="Content/Tiles/explaiNlp.png"
                                                               alt="Explainable Natural Language Processing"/></a>
                            </div>
                            <div class="col-sm-2 pageLinks">
                                <a href="wordembeddings.html"><img class="navBarImg" src="Content/Tiles/wordEmbed.png"
                                                                   alt="Word Embeddings"/></a>
                            </div>
                            <div class="col-sm-2 pageLinks">
                                <a href="interpwordembeddings.html"><img class="navBarImg"
                                                                         src="Content/Tiles/interpretWordEmbed.png"
                                                                         alt="Interpretable Word Embeddings"/></a>
                            </div>
                            <div class="col-sm-1"></div>
                        </div>
                        <br>
                    </div>
                </div>
            </div>
            <nav class="navbar-default fixed-left" id="navbar">
                <div class="container-fluid">
                    <div class="row" id="projectRow">
                        <div class="col-sm-3"></div>
                        <div class="col-sm-6">
                            <a href="index.html" id="projectName">WORD EMBEDDINGS</a>
                        </div>
                        <div class="col-sm-3"></div>
                    </div>
                </div>
            </nav>
        </div>
    </div>
    <div class="panes">
        <div class="leftpane">
            <h1></h1></div>
        <div class="middlepane">
            <div id="mainText">
                <hr>
                <div class="contentBox">
                    <div id="aboutContent">
                        <h3 class="subheading"><b>WORD EMBEDDINGS</b></h3>
                        <div class="container-fluid">
                            <div class="row">
                                <div class="col-sm-12">
                                    <hr>
                                    <p class="summary">
                                        Word embeddings is a representation of words and their contexts. These words are
                                        represented as vectors with the differing dimensions expressing different
                                        contexts of the words. These vectors are ordered lists of numbers that can
                                        represent directions and magnitudes in space. This is needed as most machine
                                        learning algorithms and deep learning architecture are unable to receive strings
                                        as input thus word embeddings replace these words as numbers that these programs
                                        can take as inputs and process.
                                    </p>
                                    <hr>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <br>
                <p class="info" id="researchParagraph">
                    These dimensions are made through the use of neural networks with methods such as Word2Vec so in
                    general these dimensions are not known. Thus, it is not known how the words are similar or differ to
                    one another. These vectors combine to create a whole matrix that represents the word embedding of
                    this vocabulary
                    <br>
                    <br>
                    From these word embeddings we can calculate the similarities of the words using cosine similarities
                    that takes the dot product of the two words that are being compared. The values returned are between
                    0 to 1 and the more related two words are the value calculated will be closer to 1. For example, the
                    words ‘cat’ and ‘dog’ would return numbers closer to 1 whereas ‘cat’ and ‘sun’ would be closer to 0
                </p>
                <hr>
                <h3 class="subheading"><b>EXAMPLE</b></h3><br>

                <div class="container-fluid">
                    <p class="info">
                        Example:
                        Let ‘cat’, ‘dog’ and ‘sun’ be vectors in the 2D plane
                        <br>
                    </p>
                    <div>
                        <p class="info"
                           style="border: 1px solid black; padding-bottom: 0px; margin-left: 40px; margin-right: 40px">
                            <br>
                            ‘Cat’ will be represented as a vector \begin{bmatrix}2\\10\end{bmatrix}
                            ‘Dog’ will be represented as a vector \begin{bmatrix}1\\10\end{bmatrix}
                            ‘Sun’ will be represented as a vector \begin{bmatrix}10\\2\end{bmatrix}
                            <br>
                        </p>
                    </div>
                    <br>
                    <p class="info">The dot product formula used to calculate the similarity of two vectors
                        $$\cos \theta = \frac{u \cdot v}{\lvert \lvert u \rvert \lvert v \rvert}$$</p>
                    <br>
                    <div class="container-fluid">
                        <div class="row">
                            <div class="col-sm-6">
                                <img class="img-fluid" src="Content/Images/vectors.PNG"
                                     alt="Vectors"/>
                            </div>
                            <div class="col-sm-6">
                                <img class="img-fluid" src="Content/Images/vectors2.PNG"
                                     alt="Vectors"/>
                            </div>
                        </div>
                    </div>
                    <hr>
                </div>
                <div class="container-fluid" style="text-align: center">
                    <p class="info">
                        In the diagram above ‘dog’, ‘cat’ and ‘sun’ are represented as vectors on a 2D Cartesian plane.
                        The
                        cosine value of α is 0.995 thus close to 1 while the value of β is 0.384 which is closer to 0,
                        thus
                        representing the similarity or difference of the words
                        <br><br>
                        Therefore, when large datasets are provided similar words will group together, thus forming
                        different groups of words that are closely related such as a group of countries or animals.
                        Word embeddings can also represent the relationship of words such as their tenses or male-female
                        relationship
                        <br>
                    <hr>
                    <br>
                    For example:
                    The figure below has vector representations for ‘male’, ‘female’, ‘bull’ and ‘cow’
                    <br><br>
                    </p>
                    <div class="row">
                        <div class="col-sm-2"></div>
                        <div class="col-sm-8">
                            <img class="img-fluid" src="Content/Images/vectorsFM.PNG"
                                 alt="VectorsFM"/>
                        </div>
                        <div class="col-sm-2"></div>
                    </div>
                    <p class="info">
                        <br>
                        The next figure shows the vector addition of $$(\vec{bull} + \vec{female}-\vec{male}) =
                        \vec{cow}$$ Since, the vector
                        from ‘male’ to ‘female’ is represented as $$(\vec{female}-\vec{male})$$
                    </p>
                    <div class="row">
                        <br>
                        <br>
                        <div class="col-sm-2"></div>
                        <div class="col-sm-8">
                            <img class="img-fluid" src="Content/Images/vectorsFM2.PNG"
                                 alt="VectorsFM2"/>
                        </div>
                        <div class="col-sm-2"></div>
                    </div>
                    <br>
                    <hr>
                    <p class="info">
                        Further reading on the various types of word embeddings and a deeper explanation on the implementations of it can be found on these links
                        <br><br>
                        <a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/" class="piyaLink">https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/</a>.
                        <a href="https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/" class="piyaLink">https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/</a>.
                        <a href="https://www.tensorflow.org/tutorials/representation/word2vec" class="piyaLink">https://www.tensorflow.org/tutorials/representation/word2vec</a>.
                    </p>
                    <hr>
                </div>
            </div>
            <h4 id="teamTitle">OUR TEAM</h4>
            <div class="container-fluid">
                <div class="row">
                    <div class="col-sm-1"></div>
                    <div class="col-sm-2 ourPics">
                        <img class="img-fluid rounded-circle us" src="Content/Images/fhb.jpg" alt="">
                        <p class="names">KYLE PATEL</p>
                    </div>
                    <div class="col-sm-2 ourPics">
                        <img class="img-fluid rounded-circle us" src="Content/Images/gl818.jpg" alt="">
                        <p class="names">GIACOMO LA SCALA</p>
                    </div>
                    <div class="col-sm-2 ourPics">
                        <img class="img-fluid rounded-circle us" src="Content/Images/ew4018.jpg" alt="">
                        <p class="names">EDWARD WILLIAMS</p>
                    </div>
                    <div class="col-sm-2 ourPics">
                        <img class="img-fluid rounded-circle us" src="Content/Images/jtb2918.png" alt="">
                        <p class="names">JAKUB BOBER</p>
                    </div>
                    <div class="col-sm-2 ourPics">
                        <img class="img-fluid rounded-circle us" src="Content/Images/fhb.jpg" alt="">
                        <p class="names">KEIGO</p>
                    </div>
                    <div class="col-sm-1"></div>
                </div>
            </div>
            <hr>
            <div id="bottomBar">
                <div class="imgHolder">
                    <a href="https://www.doc.ic.ac.uk/~pl1515/">
                        <img class="img-responsive" src="ICLpic.png" align="middle" id="iclLogo">
                    </a>
                </div>
            </div>
        </div>
        <div class="rightpane">
            <h1></h1>
        </div>
    </div>
</div>
<script>
    $("#navbar").mouseenter(function () {
        $(".collapse").collapse("show");
    });
    $("#navbar").mouseleave(function () {
        $(".collapse").collapse("hide");
    });

    $("#iclLogo").mouseenter(function () {
        $(this).animate({"max-height": "150px", "padding-top": "20px"});
    })
    $("#iclLogo").mouseleave(function () {
        $(this).animate({"max-height": "125px", "padding-top": "40px"});
    });

    $(".us").mouseenter(function () {
        $(this).animate({"opacity": "1"}, 100);
    })
    $(".us").mouseleave(function () {
        $(this).animate({"opacity": "0.7"}, 100);
    })

    $(".menuPics").mouseenter(function () {
        $(".menuPics").stop(true);
        $(".menuPics").animate({"opacity": "0.7"}, 150);
        $(this).animate({"opacity": "1"}, 150);

    })
    $(".menuPics").mouseleave(function () {
        $(".menuPics").animate({"opacity": "1"}, 300);
    })

    $(".menuHolders").mouseenter(function () {
        $(this).stop(true);
        $(this).animate({"padding": "10px"});
    })

    $(".menuHolders").mouseleave(function () {
        $(this).stop(true);
        $(this).animate({"padding": "0px"});
    })

</script>

</body>
</html>
